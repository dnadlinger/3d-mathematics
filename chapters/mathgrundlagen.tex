% Kapitel 'Mathematische Grundlagen'
\chapter{Mathematische Grundlagen}
\label{mathgrundlagen}
In den folgenden Abschnitten möchte ich die mathematischen Grundprinzipien behandeln, auf denen nahezu alle Konzepte und Operationen der 3D-Grafik aufbauen.

% Kapitel über Vektoren
% Wozu werden welche Vektoren verwendet? Erläuterung homogene Koordinaten. Schreibweisen von Vektoren.
\section{Vektoren}
Vektoren sind wohl die wichtigste Struktur in der 3D-Grafik. Sie werden nicht nur in ihrer dreidimensionalen Form eingesetzt, sondern auch in ihrer zweidimensionalen Ausprägung und um eine vierte Komponente erweitert als Vektoren mit homogenen Koordinaten (siehe Abschnitt \ref{homogeneouscoordinates}).

Trotz der universellen Verwendung gehen die benötigten Operationen nicht über den Schulstoff hinaus. Insbesondere das Skalarprodukt und das Kreuzprodukt werden sehr häufig benötigt, zum Beispiel um über den Zusammenhang
% Wie 'flacher' setzen?
$\vec{a}\cdot\vec{b} = \left|\vec{a}\right|\left|\vec{b}\right|\cos\alpha$
den von zwei Vektoren eingeschlossenen Winkel zu berechnen oder über das Kreuzprodukt eine Normale eines Polygons zu bestimmen.

Ich möchte an dieser Stelle noch kurz auf eine Eigenheit in Bezug auf Vektoren hinweisen, die im Schulunterricht bestenfalls kurz erwähnt wird: Vektoren können auf zwei Arten notiert werden, in Spaltenform
\begin{equation*}
 \vec{v} = \begin{pmatrix} x \\ y \\ z \end{pmatrix},
\end{equation*}
oder in Zeilenform
\begin{equation*}
 \vec{v} = \begin{pmatrix} x & y & z \end{pmatrix}.
\end{equation*}

Scheint der Unterschied zunächst noch rein kosmetischer Natur zu sein, zeigt sich spätestens beim Arbeiten mit Matrizen, dass die Entscheidung für eine der Formen doch erhebliche Konsequenzen nach sich zieht (siehe Abschnitt \ref{transposition}). In der Grafikprogrammierung werden beide Varianten gleichermaßen verwendet; in diesem Dokument werde ich Spaltenvektoren verwenden, um den Bezug zur Schulmathematik zu wahren.

Eingebettet in Fließtext sind Zeilenvektoren zwar wesentlich platzsparender\footnote{Zeilenvektoren sind nicht nur platzsparender, sondern auch leichter in Kommentare in 
% Lz: im
Programmcode einzubetten. Angeblich ist das einer der Gründe, warum in den Anfangszeiten der Computergrafik oft Zeilenvektoren verwendet werden.
% @cn
}, man kann einen Spaltenvektor aber als transponierten Zeilenvektor $\vec{v} = $ \textvec{x}{y}{z} anschreiben, um diesen Vorteil zu übernehmen (siehe Abschnitt \ref{transposition}).


\section{Matrizen}
Auf den ersten Blick gleichen Matrizen einfachen Tabellen von Zahlen, es gibt aber einen wesentlichen Unterschied: Für sie sind Rechenoperationen wie Addition und Multiplikation definiert.

Eine Matrix
\begin{equation}
 A = \begin{pmatrix}
   a_{11} & a_{12} & \cdots & a_{1m}\\
   a_{21} & a_{22} & \cdots & a_{2m}\\
   \vdots & \vdots & \ddots & \vdots\\
   a_{n1} & a_{n2} & \cdots & a_{nm}
 \end{pmatrix}
 \in \mathbb{R}^{m \times n}
\end{equation}
ist ein Schema aus $m \cdot n$ Elementen $a_{ij}$, die in $m$ Zeilen und $n$ Spalten angeordnet sind. Es handelt sich um eine $m \times n$-Matrix (sprich: \emph{m kreuz n}).

Die Elemente $a_{k1}, a_{k2}, \ldots, a_{km}$ (mit $1 \leq k \leq n$) bilden den $k$-ten Zeilenvektor der Matrix und analog dazu die Elemente $a_{1l}, a_{2l}, \ldots, a_{nl}$ den $l$-ten Spaltenvektor. \vgls{optimierung}{39}

Matrizen mit $m = n$ werden als \emph{quadratische Matrizen} bezeichnet und nehmen bei vielen Operationen eine Sonderstellung ein.
% Besondere Eigenschaften der quadratischen Matrizen? Einige Operationen sind nur für quadratische Matrizen definiert, ...

Zwei Matrizen heißen gleich, wenn sie von gleicher Dimension sind und in allen an gleichen Stellen stehenden Elemten übereinstimmen. \vgls{bronstein}{204}

Als \emph{Hauptdiagonale} wird die gedachte Linie von der linken oberen Ecke bis zur rechten unteren Ecke einer Matrix bezeichnet -- die Elemente der Hauptdiagonale sind also $a_{11}, a_{22}, \ldots, a_{nn}$. Quadratische Matrizen, die außerhalb der Hauptdiagonale nur Elemente mit dem Wert $0$ haben, werden als \emph{Diagonalmatrizen} bezeichnet.
% siehe http://de.wikipedia.org/wiki/Diagonalmatrix. Stimmt das?!

Grundsätzlich können die Werte der Matrix jeder algebraischen Struktur entstammen, für die Addition und Multiplikation definiert sind (also einem Halbring). Im Rahmen dieser Arbeit werde ich aber nur Matrizen über $\mathbb R$ betrachten, andere Zahlenräume sind in der 3D-Grafik nicht gebräuchlich.
% @cn
% http://en.wikipedia.org/wiki/Matrix_(mathematics)

Viele Eigenschaften und Operationen in Bezug auf Matrizen werden aber hauptsächlich im zweiten großen Einsatzgebiet der Matrizenrechnung gebraucht, dem Lösen von linearen Gleichungssystemen. An dieser Stelle soll daher nicht weiter darauf eingegangen werden.

\subsection{Addition}
Zwei Matrizen der gleichen Dimensionen $m \times n$ werden addiert, indem man jeweils die Elemente der beiden Matrizen addiert:
\begin{align}
 (A + B)_{ij} = a_{ij} + b_{ij}%\\
% \nonumber\text{mit }1 \leq i \leq m \text{ und } 1 \leq j \leq n.
\end{align}
% Quelle für Notation: http://en.wikipedia.org/wiki/Matrix_(mathematics)
mit $1 \leq i \leq m$ und $1 \leq j \leq n$.

Folglich ist das Ergebnis der Addition wiederum eine $m \times n$-Matrix.

Die Matrizenaddition ist assoziativ und kommutativ. \vgls{bronstein}{205}\ Es ist leicht zu erkennen, dass die Matrizenaddition -- analog zur Null bei der Addition von Skalaren -- ein neutrales Element besitzt: eine Matrix deren Elemente alle $0$ sind, kurz \emph{Nullmatrix} genannt.

Die Addition wird in der 3D-Grafik äußerst selten gebraucht, da sie im Gegensatz zur Multiplikation und zur Inversion keine geometrische Entsprechung hat.
% @cn

\subsection{Multiplikation}
Das Produkt $C$ zweier Matrizen $A \in \mathbb{R}^{m \times o}$ und $B \in \mathbb{R}^{o \times n}$ ist als
\begin{equation}
 c_{ij} = \sum_{k=1}^o{a_{ik} \cdot b_{kj}}
\end{equation}
mit $1 \leq i \leq m$ und $1 \leq j \leq n$ definiert ($C$ ist also $\in \mathbb{R}^{m \times n}$).

Etwas anschaulicher formuliert erhält man das $i$-te Element der $j$-ten Spalte des Ergebnisses, indem man das Skalarprodukt der $i$-ten Zeile der linken Matrix mit der $j$-ten Spalte der rechten Matrix bildet -- kurz \enquote{Zeile mal Spalte}.

Die Spaltenanzahl der linken Matrix und die Zeilenanzahl der rechten Matrix müssen also gleich sein (oben durch $o$ ausgedrückt), damit eine Multiplikation möglich ist. Das entstehende Produkt hat dabei die Dimensionen $m \times n$, also die Zeilenanzahl der linken Matrix und die Spaltenanzahl der rechten Matrix.

Ein Beispiel zur Veranschaulichung:
\begin{equation}
\begin{split}
 \begin{pmatrix}
  0 & 1 & 2 \\
  3 & 4 & 5
 \end{pmatrix}
 \cdot
 \begin{pmatrix}
  6 & 7 \\
  8 & -9 \\
  -10 & 11
 \end{pmatrix}
 &=
 \begin{pmatrix}
  0 \cdot 6 + 1 \cdot 8 + 2 \cdot (-10) & 0 \cdot 7 + 1 \cdot (-9) + 2 \cdot 11 \\
  3 \cdot 6 + 4 \cdot 8 + 5 \cdot (-10) & 3 \cdot 7 + 4 \cdot (-9) + 5 \cdot 11
 \end{pmatrix}\\
 &=
 \begin{pmatrix}
  0 + 8 - 20 & 0 - 9 + 22 \\
  18 + 32 - 50 & 21 - 36 + 55
 \end{pmatrix}
 =
 \begin{pmatrix}
   -12 & 13 \\
   0 & 40
 \end{pmatrix}
\end{split}
\end{equation}

Für die Multiplikation von quadratischen Matrizen gibt es ein neutrales Element, für das
\begin{equation}
 A \cdot E = E \cdot A = A
\end{equation}
gilt. $E$ wird als \emph{Einheitsmatrix} bezeichnet und ist eine Diagonalmatrix, deren Elemente entlang der Hauptdiagonale alle den Wert $1$ haben. Es gibt natürlich für alle verschiedenen Größen $n \times n$ jeweils eine eigene Einheitsmatrix $E_n$. Im Normalfall ist aber ohnehin ersichtlich, welche Dimensionen die verwendete Einheitsmatrix haben muss, deswegen wird in der Praxis meistens auf den Index verzichtet.

Sofern die Bedingungen bezüglich der Dimensionen der Operanden erfüllt sind, ist die Matrizenmultiplikation assoziativ. Das Kommutativgesetz gilt aber nicht! \vgls{bronstein}{205}

\subsection{Transformation von Vektoren}
Da ein $n$-dimensionaler Spaltenvektor ja im Grunde nichts anderes ist als eine $n \times 1$-Matrix, kann man natürlich auch eine Matrix mit einem Vektor multiplizieren. Wenn es sich dabei um eine quadratische $n \times n$-Matrix handelt, ist das Ergebnis wiederum ein $n$-dimensionaler Vektor. Rechnet man eine solche Multiplikation in ihrer allgemeinen Form aus
\begin{equation}
 \begin{pmatrix}
  m_{11} & m_{12} & m_{13} \\
  m_{21} & m_{22} & m_{23} \\
  m_{31} & m_{32} & m_{33}
 \end{pmatrix} \cdot
 \begin{pmatrix}
  x \\ y \\ z
 \end{pmatrix} =
 \begin{pmatrix}
  m_{11} \cdot x + m_{12} \cdot y + m_{13} \cdot z \\
  m_{21} \cdot x + m_{22} \cdot y + m_{23} \cdot z \\
  m_{31} \cdot x + m_{32} \cdot y + m_{33} \cdot z
 \end{pmatrix},
\end{equation}
so bemerkt man, dass die Zeilen der Matrix jeweils die Koeffizienten einer Linearkombination der Komponenten des Vektors darstellen. Es kann daher jede lineare Abbildung zwischen zwei (endlichdimensionalen) Vektorräumen als Matrix dargestellt werden. Die Multiplikation einer Matrix mit einem Vektor wird aus diesem Grund auch als \emph{Transformation} des Vektors bezeichnet. Wie man auch leicht durch Einsetzen überprüfen kann, ändert eine Multiplikation mit der Einheitsmatrix den Vektor naheliegenderweise nicht.
% http://de.wikipedia.org/wiki/Lineare_Abbildung

\label{homogeneouscoordinates}
Allerdings reicht eine Linearkombination der Komponenten nicht aus, um alle in der 3D-Grafik benötigten Transformationen darzustellen -- manchmal ist es nötig, alle Komponenten des Vektors durch eine von ihnen zu dividieren oder von den Komponenten unabhängige Werte zu addieren. Um auch diesen größeren Raum der \emph{affinen Transformationen} in Matrixform darstellen zu können, bedient man sich eines Tricks und erweitert den Vektor um eine zusätzliche Dimension, die meistens mit $w$ bezeichnet wird. Man erhält die sogenannten \emph{homogenen Koordinaten} -- das Pendant mit homogenen Koordinaten zu einem dreidimensionalen Vektor ist also ein Vektor im $\mathbb{R}^4$.

Um einen Vektor mit \enquote{normalen} kartesischen Koordinaten in einen Vektor mit homogenen Koordinaten zu überführen, setzt man einfach $w = 1$:
\begin{equation}
 \begin{pmatrix} x & y & z \end{pmatrix}^T \rightarrow \begin{pmatrix} x & y & z & 1 \end{pmatrix}^T
\end{equation}

Für den umgekehrten Fall dividiert man zuerst alle Komponenten durch $w$:
\begin{equation}
 \begin{pmatrix} x & y & z & w \end{pmatrix}^T \rightarrow \begin{pmatrix} \frac{x}{w} & \frac{y}{w} & \frac{z}{w} & 1 \end{pmatrix}^T
\end{equation}
Danach kann man aus den ersten drei Komponenten des Vektors wieder einen Vektor mit kartesischen Koordinaten bilden, $w$ kann also einfach weggelassen werden.
% Beispiele nach http://de.wikipedia.org/wiki/Homogene_Koordinaten

In der Grafikprogrammierung werden homogenen Koordinaten meist nur dort verwendet, wo sie gebraucht werden, wenn also affine Transformationen durchgeführt werden sollen. Bei allen anderen Berechnungen wird mit kartesischen Koordinaten gearbeitet, zwischen den beiden Koordinatentypen wird nach Bedarf hin- und hergesprungen. Auch wenn diese Vorgehensweise aus mathematischer Sicht nicht konsequent ist, stellt sie in der Praxis einen sinnvollen Kompromiss zwischen Performance\footnote{Performance: Leistung bzw. Geschwindigkeit eines Computerprogramms} und Flexibilität dar.
% @cn
% Hyperebene: Eberly (3D game engine design), S. 9 (2.1.4 Homogeneous Transformations)
% Verwendung von w in Robotik-Matrizen?

\label{transformationmatrixcombination}
Die Darstellung aller Transformationen als Matrizen hat den großen Vorteil, dass beliebig viele Transformationen mittels Multiplikation in eine Matrix kombiniert werden können, die dann wiederum auf viele Vektoren angewendet werden kann. Dies ist möglich, weil die Matrizenmultiplikation assoziativ ist:
\begin{equation}
 \left( M_3 \cdot \left( M_2 \cdot \left( M_1 \cdot \vec v \right) \right) \right) = \left( M_3 \cdot M_2 \cdot M_1 \right) \cdot \vec v
\end{equation} 

Durch diese Universalität von Matrizen ist es möglich, bestimmte zeitintensive Berechnungen in der Computergrafik sehr effizient zu gestalten (siehe Kapitel \ref{coordinatesystems}).
% @Citation?!

\subsection{Transposition}
\label{transposition}
Bei der Transposition werden die Zeilen und Spalten einer Matrix vertauscht, die Matrix wird quasi entlang ihrer Hauptdiagonale gespiegelt:
\begin{equation}
 (A^T)_{ij} = a_{ji}
\end{equation} 
Folglich wird eine $m \times n$-Matrix zu einer $n \times m$-Matrix. Wird eine $n \times 1$-Matrix, also ein Spaltenvektor, transponiert, ergibt sich daher eine $1 \times n$-Matrix, der entsprechende Zeilenvektor.

Beispiel:
\begin{equation}
  \begin{pmatrix}
    1 & -2 & 3 \\
    4 & 5 & -6
  \end{pmatrix}^T
  =
  \begin{pmatrix}
    1 & 4 \\
    -2 & 5 \\
    3 & -6
  \end{pmatrix}
\end{equation}

Wie sich leicht überprüfen lässt, hebt sich die Transposition auf, wenn sie zwei Mal auf eine Matrix angewendet wird. Es gilt also
\begin{equation}
 (A^T)^T = A.
\end{equation}

Bezüglich der Addition ist die Transposition distributiv, bezüglich der Multiplikation distributiv unter Umkehrung der Reihenfolge. \vgls{bronstein}{206}\ Es gelten also die Gleichungen
\begin{equation}
 (A + B)^T = A^T + B^T
\end{equation}
und
\begin{equation}
\label{transpositionmultiplication}
 (A \cdot B)^T = B^T \cdot A^T.
\end{equation}

Der Zusammenhang aus Gleichung \ref{transpositionmultiplication} zieht einen wesentlichen Unterschied in der Verwendung von Spalten- und Zeilenvektoren nach sich. Dazu ein kleines Beispiel: Gegeben sei ein Spaltenvektor $\vec s$ und eine Matrix $A$, durch Multiplikation aus mehreren Teilmatrizen $A_1, A_2, \dots, A_n$ kombiniert, die jeweils einer Transformation entsprechen. Der Vektor wird nun von \emph{rechts} an die Matrix multipliziert:
\begin{equation}
 \vec{s'} = A \cdot \vec{s} = A_1 \cdot A_2 \cdot \ldots \cdot A_n \cdot \vec{s} = A_1 \cdot A_2 \cdot \ldots \cdot \left( A_n \cdot \vec{s} \right).
\end{equation}
Wie durch die Klammernsetzung angedeutet, verhält sich der Ergebnisvektor $\vec{s'}$, als hätte man die Teiltransformationen der Reihe nach beginnend mit $A_n$, also der zuletzt zu $A$ multiplizierten Matrix, auf den Vektor angewendet.

Wird nun statt einem Spaltenvektor ein Zeilenvektor $\vec z = \vec{s}^T$ verwendet, ergibt sich aus Gleichung \ref{transpositionmultiplication} (wie auch aus den für die Multiplikation nötigen Dimensionen der Argumente), dass ein Zeilenvektor von \emph{links} an die \emph{transponierte} Matrix multipliziert werden muss. Wenn diese wieder aus $n$ Teiltransformationen zusammengesetzt wird, reicht es nicht, die einzelnen Matrizen zu transformieren, es muss zusätzlich noch die Multiplikationsreihenfolge der Teilmatrizen umgekehrt werden:
\begin{equation}
 \vec{s'}^T = \vec{s}^T \cdot A^T = \vec{s}^T \cdot A_n^T \cdot A_{n-1}^T \cdot \ldots \cdot A_1^T.
\end{equation}
Dies ist insofern erwähnenswert, als in der 3D-Grafik zwei große Standards existieren, von denen einer Spaltenvektoren, der andere Zeilenvektoren vorsieht (mehr dazu in Kapitel \ref{direct3dopengl}) und manche Algorithmen daher nicht 1:1 von einem Standard auf den anderen übertragbar sind.
% Welche?

% Übrigens entspricht die transponierte Matrix bei Matrizen über $\mathbb R$ (und um diese soll es hier ausschließlich gehen) der \emph{adjungierten Matrix}. Aus diesem Grund werden die beiden Begriffe in der Fachliteratur zur Grafikprogrammierung gelegentlich synonym verwendet. Diese ist allerdings nicht mit der \emph{Adjunkten} zu verwechseln, die bei der Berechnung der inversen Matrix Verwendung findet.
% Quelle: http://de.wikipedia.org/wiki/Matrix_(Mathematik)#Die_transponierte_Matrix

\subsection{Determinante}
Die Determinante ist eine spezielle Funktion, die einer quadratischen Matrix $A$ eine Zahl $\det A$ zuordnet. Das Ergebnis kann auf verschiedene Weisen interpretiert werden, die bekannteste Verwendung ist sicherlich zur Volumenberechnung in der Vektorrechnung (der Betrag der Determinante dreier Vektoren aus dem $\mathbb R^3$ entspricht dem Spatprodukt).
% @cn (Wikipedia)

Wenn die Matrix als Koeffizientenmatrix eines linearen Gleichungssystems aufgefasst wird, besitzt dieses genau dann eine eindeutige Lösung, wenn ihre Determinante ungleich $0$ ist (solche Matrizen werden als \emph{reguläre Matrizen} bezeichnet). \vgls{optimierung}{56}

In der Grafikprogrammierung ist die Determinante selbst praktisch bedeutungslos, allerdings wird sie für ein Verfahren zur Inversion von Matrizen benötigt (siehe Abschnitt \ref{inversion}).

Die Determinante einer $2 \times 2$-Matrix
\begin{equation*}
 A = \begin{pmatrix}
      a & b \\
      c & d
     \end{pmatrix}
\end{equation*}
lautet
\begin{equation}
 \det A = ad - cb.
\end{equation}

Mit Hilfe des Laplace'schen Entwicklungssatzes kann die Berechnung von Determinanten aller höheren Dimensionen auf $2 \times 2$-Determinanten zurückgeführt werden. Dazu wird das Konzept der \emph{Minoren} eingeführt: Der Minor $M_{ij}$ einer Matrix $A \in \mathbb{R}^{n \times n}$ ist die Determinante der $(n-1) \times (n-1)$-Untermatrix, die durch Streichen der $i$-ten Zeile und der $j$-ten Spalte von A entsteht. Das Produkt
\begin{equation}
\label{cofactor}
 \tilde{a}_{ij} = (-1)^{i+j} M_{ij}
\end{equation}
wird als \emph{Kofaktor} von A bezeichnet. Nach dem Laplace'schen Erweiterungssatz kann die Determinante einer Matrix als Summe der Produkte ihrer Elemente mit den zugehörigen Kofaktoren \enquote{nach} einer beliebigen Zeile oder Spalte entwickelt werden \vgls{bronstein}{202}:
\begin{equation}
 \det A = \sum_{i=1}^n a_{ik} \tilde{a}_{ik} = \sum_{j=1}^n a_{kj} \tilde{a}_{kj}
\end{equation}
(mit $1 \leq k \leq n$).

Hieraus lässt sich schon eine Eigenschaft in Bezug auf die Transposition erkennen: Die Determinante einer Matrix verändert sich nicht, wenn man die Matrix transponiert. \vgls{bronstein}{201}\ Es gilt also:
\begin{equation}
\label{transposeddeterminant}
 \det A = \det A^T
\end{equation}

Nach diesem Verfahren lässt sich eine komplette Formel für die Determinante ableiten, was für die Geschwindigkeit der Berechnung am Computer von Vorteil ist. Für eine $3 \times 3$-Matrix $B$ erhält man beispielsweise nach dem Vereinfachen
\begin{equation}
 \det B = b_{11} b_{22} b_{33} + b_{12} b_{23} b_{31} + b_{13} b_{21} b_{32} - b_{13} b_{22} b_{31} - b_{12} b_{21} b_{33} - b_{11} b_{23} b_{32}.
\end{equation}
Wie man leicht erkennen kann, wächst die Komplexität dieses Verfahrens mit steigender Dimension ziemlich schnell. Zur Berechnung höherdimensionaler Determinanten können daher andere algorithmische Verfahren sinnvoller sein. In der 3D-Programmierung sind diese aber mehr oder weniger bedeutungslos, da es hier auf schnelle Berechenbarkeit bei kleinen Dimensionen ankommt.
% http://de.wikipedia.org/wiki/Determinante_(Mathematik)

\subsection{Inverse Matrix}
\label{inversion}
Eine Matrix $A$ kann eine inverse Matrix $A^{-1}$ besitzen, für die
\begin{equation}
 A \cdot A^{-1} = A^{-1} \cdot A = E
\end{equation}
gilt. $A^{-1}$ wird auch kurz als Inverse bezeichnet. Wie oben schon angemerkt, existiert nur zu regulären Matrizen eine inverse Matrix, also zu quadratischen Matrizen, deren Determinante ungleich $0$ ist. \vgls{bronstein}{206}

Nicht-quadratische Matrizen können prinzipiell keine inverse Matrix besitzen.
% @more
% allerdings können unter Umständen ... http://en.wikipedia.org/wiki/Moore-Penrose_pseudoinverse

Im Bezug auf die Rolle der Matrix als Darstellung einer geometrischen Transformation ist es wichtig zu erkennen, dass die Multiplikation eines Vektors mit der inversen Matrix dem \emph{Rückgängig machen} der Transformation entspricht. Hat man also beispielsweise einen Vektor $\vec v$ mit der Matrix $A$ transformiert, erhält man nach einer Multiplikation mit der Inversen $A^{-1}$ wieder den ursprünglichen Vektor. Dies lässt sich auch direkt aus der Definition ableiten: $A^{-1} \cdot ( A \cdot \vec v ) = (A^{-1} \cdot A) \cdot \vec v = E \cdot \vec v = \vec v$.

Eine singuläre, also nicht invertierbare Matrix ist oft auf den ersten Blick erkennbar, wenn man die Auswirkungen der Matrix auf einen Vektor betrachtet. Beispielsweise kann zu der Matrix
\begin{equation}
 \begin{pmatrix}
  1 & 0 & 0 \\
  0 & 0 & 0 \\
  0 & 0 & 1
 \end{pmatrix}
\end{equation}
keine inverse Matrix existieren, denn wenn man einen Vektor mit dieser Matrix transformiert, geht die Information aus der $y$-Koordinate \enquote{verloren}. Somit ist es nachträglich nicht mehr möglich, den ursprünglichen Vektor wiederherzustellen.

In Bezug auf Transposition gilt für die Inversion \vglr{wiki:inverse}:
\begin{equation}
 (A^{-1})^T = (A^T)^{-1}.
\end{equation}
% @cn

Im Wesentlichen gibt es zwei Verfahren, um die Inverse zu einer Matrix zu berechnen. Für die folgenden Betrachtungen soll
\begin{equation*}
 A = \begin{pmatrix}
  1 & 2 & 0 \\
  2 & 3 & 0 \\
  3 & 4 & 1
 \end{pmatrix}
\end{equation*}
als Ausgangsmatrix dienen.

Die erste Variante beruht auf dem Gauß-Eliminationsverfahren. Dazu schreibt man die Ausgangsmatrix als Blockmatrix $(A|E)$ mit der Einheitsmatrix an:
\begin{equation}
 \left(\begin{array}{ccc|ccc}
    1 & 2 & 0 &  1 & 0 & 0 \\
    2 & 3 & 0 &  0 & 1 & 0 \\
    3 & 4 & 1 &  0 & 0 & 1
  \end{array}\right)
\end{equation}
Danach wendet man den Gauß-Jordan-Algorithmus auf die Blockmatrix an, bis auf der linken Seite die Einheitsmatrix steht, was in diesem Fall zu folgendem Ergebnis führt:
\begin{equation}
  \left(\begin{array}{ccc|ccc}
    1 & 0 & 0  & -3 & 2 & 0 \\
    0 & 1 & 0  & 2 & -1 & 0 \\
    0 & 0 & 1  & 1 & -2 & 1
  \end{array}\right)
\end{equation}
Auf der rechten Seite kann die inverse Matrix $A^{-1}$ nun direkt abgelesen werden.
% Lz: Fußnote mit Gauß-Jordan-Algorithmus
% Beispiel von http://de.wikipedia.org/wiki/Regul%C3%A4re_Matrix, korrekt?

Mit diesem Verfahren ist die inverse Matrix recht einfach händisch zu berechnen. Es hat allerdings den Nachteil, dass sich daraus keine \enquote{fertige} Formel ableiten lässt und es daher für den Einsatz in der Grafikprogrammierung nicht ausreichend schnell zu berechnen ist\footnote{Dies gilt wiederum nur für relativ kleine Matrizen, wie sie in der 3D-Grafik ausschließlich zur Anwendung kommen. Bei höherdimensionalen Matrizen führen andere algorithmische Ansätze in der Regel schneller zum Ziel.}.

Daher wird in der 3D-Programmierung meist auf die zweite Variante zurückgegriffen, nach der die Inverse zur Matrix $A$ folgendermaßen definiert ist:
\begin{equation}
 A^{-1} = \frac{1}{\det A} \cdot \adj A
\end{equation}

$\adj A$ bezeichnet hier die \emph{Adjunkte} der Matrix A, die auch \emph{komplementäre Matrix} genannt wird. Sie besteht aus der transponierten Matrix der Kofaktoren (siehe Gleichung \ref{cofactor}), es gilt also:
\begin{equation}
 \adj A = \begin{pmatrix}
   \tilde a_{11} & \tilde a_{21} & \cdots & \tilde a_{n1}\\
   \tilde a_{12} & \tilde a_{22} & \cdots & \tilde a_{n2}\\
   \vdots & \vdots & \ddots & \vdots\\
   \tilde a_{1n} & \tilde a_{2n} & \cdots & \tilde a_{nn}
 \end{pmatrix}.
\end{equation}
% Lz: Beispiel

Mit Hilfe dieses Zusammenhangs lassen sich Formeln für Matrizen beliebiger Dimension herleiten. Dabei wird allerdings bald deutlich, dass die Komplexität bzw. die Anzahl der nötigen Rechenoperationen mit der Erhöhung der Dimensionen sehr schnell ansteigt:

Ergibt sich für die Inverse einer $2 \times 2$-Matrix
\begin{equation}
 B = \begin{pmatrix}
  a & b \\
  c & d
 \end{pmatrix}
\end{equation}
noch der relativ übersichtliche Ausdruck
\begin{equation}
 B^{-1} = \frac{1}{ad-bc} \cdot
 \begin{pmatrix}
  d & -b \\
  -c & a
 \end{pmatrix},
\end{equation}
führt das Verfahren für eine $3 \times 3$-Matrix
\begin{equation}
 C = \begin{pmatrix}
  a & b & c \\
  d & e & f \\
  g & h & i
 \end{pmatrix}
\end{equation}
schon zu der wesentlich komplizierteren Formel
\begin{equation}
 C^{-1} = \frac{1}{\det C} \cdot
 \begin{pmatrix}
  ei - hf & ch - bi & bf - ce \\
  fg - di & ai - cg & cd - af \\
  dh - eg & bg - ah & ae - bd
 \end{pmatrix}.
\end{equation}

In Bezug auf die 3D-Grafik am Computer bleibt noch festzustellen, dass die Inversion im Vergleich zu anderen Operationen (etwa der Matrizenmultiplikation) recht rechenaufwändig ist und daher nicht allzu oft ausgeführt werden sollte (siehe Abschnitt \ref{performance}).

\section{Quaternionen}
\label{quaternionmath}
Der Körper der komplexen Zahlen $\mathbb C$ als Erweiterung der reellen Zahlen ist ja bereits aus dem Schulunterricht bekannt. Neben einigen anderen wichtigen Eigenschaften wie der algebraischen Abgeschlossenheit\footnote{Dies ist der Inhalt des berühmten Fundamentalsatzes der Algebra, der Ende des 18. Jahrhunderts von Carl Friedrich Gauß bewiesen wurde.}, haben die komplexen Zahlen die angenehme Eigenschaft, dass sie gleichzeitig ein zweidimensionaler reeller Vektorraum sind und sich über sie viele Brücken zwischen Geometrie und Algebra schlagen lassen.
% @cn

Es verwundert deshalb nicht, dass viele Mathematiker versuchten, ein dreidimensionales Äquivalent zu den komplexen Zahlen zu finden. Einer von ihnen war auch der irische Mathematiker Hamilton, der wie alle anderen zunächst an dem Problem scheiterte, dass für \enquote{dreidimensionale komplexe Zahlen} keine vernünftigen Rechenregeln zu finden waren. Schließlich kam ihm die entscheidende Idee (angeblich, während er mit seiner Frau über die Broom Bridge in Dublin spazierte), das Konzept um eine zusätzliche vierte Komponente zu erweitern. (zu diesem und den folgenden Absätzen \vglst{quaternionrotation}{ 29-37})

Eine \emph{Quaternion}\footnote{In der Literatur wird der Begriff sowohl im Maskulinum, als auch im Femininum gebraucht. Ich habe mich in diesem Dokument dazu entschlossen, laut Duden die weibliche Form zu verwenden.} ist also eine Zahl der Form
\begin{equation}
 q = w + xi + yj + zk = \left[ w; \begin{pmatrix} x & y & z \end{pmatrix}^T \right] \in \mathbb H
\end{equation}
mit $w, x, y, z \in \mathbb R$, wobei $j$ und $k$ analog zu dem von den komplexen Zahlen bekannten $i$ zwei weitere voneinander verschiedene imaginäre Einheiten sind. Der Raum aller Quaternionen wird in Anlehnung an den Namen ihres geistigen Vaters als $\mathbb H$ bezeichnet.

Für $i$, $j$ und $k$ gilt der Zusammenhang
\begin{equation}
 i^2 = j^2 = k^2 = ijk = -1,
\end{equation} 
der auch als Hamilton'sche Regel bekannt ist.

Zwischen den drei imaginären Einheiten besteht eine zyklische Abhängigkeit:
\begin{align}
 i \cdot j = -j \cdot i &= k \\
 j \cdot k = -k \cdot j &= i \\
 k \cdot i = -i \cdot k &= j
\end{align}
Die Multiplikation der imaginären Einheiten und damit auch der Quaternionen generell ist also nicht kommutativ!

Der (reelle) Betrag einer Quaternion ist analog zu den komplexen Zahlen und den Vektoren als 
\begin{equation}
 \left| q \right| = \sqrt{w^2 + x^2 + y^2 + z^2}
\end{equation}
definiert. Eine Quaternion mit $\left| q \right| = 1$ wird als \emph{Einheitsquaternion} bezeichnet.

Wie schon durch die Zeichen $x$, $y$ und $z$ angedeutet, kann man eine Quaternion auch als Struktur mit einem \emph{skalaren Anteil} $w$ und einem \emph{vektoriellen Anteil} \textvec{x}{y}{z} gesehen werden. Eine alternative Schreibweise für die Quaternion $q = w + v_{x}i + v_{y}j + v_{z}k$ lautet daher $\left[w; \vec v \right]$. Quaternionen mit dem Realteil $0$ heißen auch \emph{reine Quaternionen}, eine reine Quaternion $q = 0 + v_{x}i + v_{y}j + v_{z}k$ kann jederzeit als Vektor $ \vec v = \begin{pmatrix} x & y & z \end{pmatrix}^T \in \mathbb R^3$ ausgedrückt werden und umgekehrt. Die einem Vektor $\vec v$ zugeordnete reine Quaternion wird als $\hat{v}$ abgekürzt \vglsd{script:spain}{92}{rotationissues}{4}:
\begin{equation}
 \hat{v} = \left[ 0; \vec v \right]
\end{equation} 

Die konjugierte Quaternion zu einer Quaternion $q = w + v_{x}i + v_{y}j + v_{z}k$ ist als
\begin{equation}
 \overline{q} = w - v_{x}i - v_{y}j - v_{z}k = \left[ w; -\vec v \right]
\end{equation}
definiert. Die Konjugation ändert also das Vorzeichen des Vektorteils der Quaternion, lässt den Skalarteil aber unverändert.

Zwei Quaternionen $q_1$ und $q_2$ werden \emph{addiert} und \emph{subtrahiert}, indem man so wie bei Vektoren und komplexen Zahlen die einzelnen Koeffizienten addiert bzw. subtrahiert:
\begin{equation}
\begin{split}
 q_1 \pm q_2 &= w_1 + x_{1}i + y_{1}j + z_{1}k \pm \left( w_2 + x_{2}i + y_{2}j + z_{2}k \right) \\
 &= w_1 \pm w_2 + \left( x_1 \pm x_2 \right) \cdot i + \left( y_1 \pm y_2 \right) \cdot j + \left( z_1 \pm z_2 \right) \cdot z \\
 &= \left[ w_1 \pm w_2; \vec{v_1} \pm \vec{v_2} \right]
\end{split}
\end{equation}

Eine Quaternion $q$ kann nach den ganz gewöhnlichen Rechenregeln mit einem \emph{Skalar} $\lambda$ multipliziert werden:
\begin{equation}
 \lambda \cdot q = \lambda \cdot (w + xi + yj + zk) = \lambda w + \lambda xi + \lambda yj + \lambda zk = \left[ \lambda w; \lambda \cdot \vec v \right]
\end{equation}

Zwei Quaternionen $q_1$ und $q_2$ können ebenfalls ganz gewöhnlich miteinander \emph{multipliziert} werden. Ausmultiplizieren (unter Beachtung der Hamilton-Regeln) und Anwenden des Distributivgesetzes ergibt:
\begin{equation}
\begin{split}
 q_1 \cdot q_2 &= ( w_1 + x_{1}i + y_{1}j + z_{1}k ) \cdot ( w_2 + x_{2}i + y_{2}j + z_{2}k ) \\
 &= w_1 w_2 - ( x_1 x_2 + y_1 y_2 + z_1 z_2 ) \\
    & + ( w_1 x_2 + w_2 x_1 + y_1 z_2 - y_2 z_1 ) \cdot i \\
    & + ( w_1 y_2 + w_2 y_1 + x_2 z_1 - x_1 z_2 ) \cdot j \\
    & + ( w_1 z_2 + w_2 z_1 + x_1 y_2 - x_2 y_1 ) \cdot k
\end{split}
\end{equation}
Fasst man den Imaginärteil der Quaternion als Vektor auf, kann man die Multiplikation auch wie folgt auflösen:
\begin{equation}
 \label{quaternionmultiplicationvector}
 q_1 \cdot q_2 = \left[ \left( w_1 w_2 - \vec{v_1} \cdot \vec{v_2} \right); \vec{v_1} \times \vec{v_2} + w_1 \cdot \vec{v_2} + w_2 \cdot \vec{v_1} \right]
\end{equation}
Das Produkt zweier Quaternionen ist nicht kommutativ, aber assoziativ und distributiv.

Ähnlich wie die Matrizenmultiplikation bezüglich der Transposition (siehe Gleichung \ref{transpositionmultiplication}) verhält sich auch die Quaternionenmultiplikation bezüglich der Konjugation distributiv unter Vertauschung der Reihenfolge:
\begin{equation}
 \label{quaternionconjugationmultiplication}
 \overline{( q_1 \cdot q_2 )} = \overline{q_2} \cdot \overline{q_1}
\end{equation} 

Wenn man eine Quaternion mit ihrer konjugierten Quaternion multipliziert
\begin{equation}
\begin{split}
 q \cdot \overline{q} &= \left[ w; \vec v \right] \cdot \left[ w; -\vec v \right] \\
 & = \left[ \left( w \cdot w - \vec{v} \cdot (-\vec{v}) \right); \vec{v} \times (-\vec{v}) + w \cdot \left(-\vec{v}\right) + w \cdot \vec{v} \right],
\end{split}
\end{equation}
fällt auf, dass der gesamte Imaginärteil des Ergebnisses Null wird. Übrig bleibt, wie auch bei den komplexen Zahlen, eine reelle Zahl, die dem Quadrat des Betrags der Quaternion entspricht:
\begin{equation}
\begin{split}
 q \cdot \overline{q} &= w \cdot w + \vec v \cdot \vec v \\
 &= w^2 + x^2 + y^2 + z^2 \\
 &= \left| q \right|^2
\end{split}
\end{equation}

Die \emph{Division} ist für Quaternionen nicht definiert, aber analog zu den Matrizen gibt es zu einer Quaternion $q$ ein \emph{inverses Element} $q^{-1}$, für das gilt:
\begin{equation}
 q \cdot q^{-1} = q^{-1} \cdot q = 1
\end{equation}
Diese \emph{Inverse} einer Quaternion $q$ ist als
\begin{equation}
 q^{-1} = \frac{1}{\left| q \right|} \cdot \overline{q}
\end{equation}
definiert. Für den speziellen Fall, dass der Betrag einer Quaternion $1$ ist, es sich also um eine Einheitsquaternionen handelt, gilt also:
\begin{equation}
 q^{-1} = \overline{q}
\end{equation}